{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4224963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "     ------------------------------------ 494.8/494.8 kB 373.6 kB/s eta 0:00:00\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "     ------------------------------------ 134.8/134.8 kB 568.9 kB/s eta 0:00:00\n",
      "Collecting tqdm>=4.66.3\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from datasets) (22.0)\n",
      "Collecting requests>=2.32.2\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Collecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "     -------------------------------------- 515.3/515.3 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     -------------------------------------- 193.6/193.6 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.14-cp310-cp310-win_amd64.whl (451 kB)\n",
      "     ------------------------------------ 451.3/451.3 kB 940.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.3/116.3 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "     ---------------------------------------- 43.8/43.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (22.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp310-cp310-win_amd64.whl (45 kB)\n",
      "     ---------------------------------------- 46.0/46.0 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 86.8/86.8 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, tqdm, requests, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 huggingface-hub-0.33.4 multidict-6.6.3 multiprocess-0.70.16 propcache-0.3.2 requests-2.32.4 tqdm-4.67.1 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.27 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.27 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.27 requires requests==2.28.1, but you have requests 2.32.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31f4ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from pdfplumber) (9.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (39.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\greeshma\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42cca4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'shrijayan/gov_myscheme' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset and remove the problematic 'pdf' column\n",
    "dataset = load_dataset(\"shrijayan/gov_myscheme\", split=\"train\", trust_remote_code=True)\n",
    "dataset = dataset.remove_columns(['pdf'])  # This prevents pdf decoding\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Preview\n",
    "print(df.columns)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba184cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extraction complete. Saved to andhra_schemes_extracted.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your Andhra text files\n",
    "data_path = \"Indian_govt_schemes/andhra-pradesh\"\n",
    "\n",
    "# Result list\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(data_path, filename), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "            # --- Basic Cleaning ---\n",
    "            lines = content.splitlines()\n",
    "            lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "            # --- Extract Fields ---\n",
    "            scheme_name = lines[0] if lines else \"Unknown\"\n",
    "            \n",
    "            # Use keyword matches to extract summaries\n",
    "            eligibility_lines = [line for line in lines if \"eligible\" in line.lower() or \"income\" in line.lower()]\n",
    "            benefit_lines = [line for line in lines if \"benefit\" in line.lower() or \"support\" in line.lower() or \"reimbursement\" in line.lower()]\n",
    "\n",
    "            eligibility_summary = \" \".join(eligibility_lines[:2]) if eligibility_lines else \"\"\n",
    "            benefit_summary = \" \".join(benefit_lines[:2]) if benefit_lines else \"\"\n",
    "\n",
    "            # Store the results\n",
    "            data.append({\n",
    "                \"State\": \"Andhra Pradesh\",\n",
    "                \"Filename\": filename,\n",
    "                \"Scheme Name\": scheme_name,\n",
    "                \"Eligibility\": eligibility_summary,\n",
    "                \"Benefit\": benefit_summary,\n",
    "                \"Raw Content\": content\n",
    "            })\n",
    "\n",
    "# Convert to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"andhra_schemes_extracted.csv\", index=False)\n",
    "print(\"✅ Extraction complete. Saved to andhra_schemes_extracted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827eae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all state schemes to all_states_schemes_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_fields(text):\n",
    "    lines = text.splitlines()\n",
    "    lines = [line.strip() for line in lines if line.strip() and \"Table of Contents\" not in line]\n",
    "\n",
    "    # Scheme Name\n",
    "    scheme_name = next((line for line in lines if re.search(r'scheme|yojana|vidya|mission', line, re.IGNORECASE)), \"Unknown\")\n",
    "\n",
    "    # Eligibility\n",
    "    eligibility_lines = [line for line in lines if re.search(r'eligible|income|limit|category|bpl|poverty', line, re.IGNORECASE)]\n",
    "    eligibility_summary = \" \".join(eligibility_lines[:2]) if eligibility_lines else \"\"\n",
    "\n",
    "    # Benefit\n",
    "    benefit_lines = [line for line in lines if re.search(r'benefit|support|reimbursement|financial aid|money|help', line, re.IGNORECASE)]\n",
    "    benefit_summary = \" \".join(benefit_lines[:2]) if benefit_lines else \"\"\n",
    "\n",
    "    return scheme_name, eligibility_summary, benefit_summary, \"\\n\".join(lines)\n",
    "\n",
    "# Main processing\n",
    "root_dir = \"Indian_govt_schemes\"  # Main folder with state subfolders\n",
    "all_data = []\n",
    "\n",
    "for state_folder in os.listdir(root_dir):\n",
    "    state_path = os.path.join(root_dir, state_folder)\n",
    "    if os.path.isdir(state_path):\n",
    "        for filename in os.listdir(state_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(state_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    scheme_name, eligibility, benefit, raw = extract_fields(content)\n",
    "\n",
    "                    all_data.append({\n",
    "                        \"State\": state_folder.replace(\"-\", \" \").title(),\n",
    "                        \"Filename\": filename,\n",
    "                        \"Scheme Name\": scheme_name,\n",
    "                        \"Eligibility\": eligibility,\n",
    "                        \"Benefit\": benefit,\n",
    "                        \"Raw Content\": raw\n",
    "                    })\n",
    "\n",
    "# Save to CSV\n",
    "df_all = pd.DataFrame(all_data)\n",
    "df_all.to_csv(\"all_states_schemes_cleaned.csv\", index=False)\n",
    "print(\"✅ Saved all state schemes to all_states_schemes_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b203ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
